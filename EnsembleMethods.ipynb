{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). \n",
    "\n",
    " **What you need to remember:**\n",
    "\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
    "- Write code in the designated areas using Python 3 only\n",
    "- Do not modify the code outside of the designated areas\n",
    "- In some cases you will also need to explain the results. There will also be designated areas for that. \n",
    "\n",
    "Fill in your **NAME** and **AEM** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Andreas Andreadis\"\n",
    "AEM = \"2729\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1da26a63e48275bf64ed3608a92f75ac",
     "grade": false,
     "grade_id": "cell-28329c89a3d9ebb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e378be0e0c4ed85ebc6bcc53e256aa5",
     "grade": false,
     "grade_id": "cell-17ca53188deb1a2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fb9c467676bec1973d8e90bb815e23f",
     "grade": false,
     "grade_id": "cell-1a33a1efbf02238c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa2d11566658cb0f6d5ea19212a619de",
     "grade": false,
     "grade_id": "cell-7210caf6b2891007",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c426c8680b4b28c9595139bec1d32f27",
     "grade": false,
     "grade_id": "cell-a413577b7685bfbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e073a9f617d5e2e192ef70d370f000b",
     "grade": false,
     "grade_id": "cell-cbadea205635117c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5c0ed51001ec5b7e6cdcef1303473e7",
     "grade": false,
     "grade_id": "cell-4f509bca5cb87e84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure and accuracy of your models.\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d56eafac9ce59f34521ab931a24e9c8",
     "grade": false,
     "grade_id": "cell-db7468662add40fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three estimators/classifiers. Test both soft and hard voting and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf11fd8382f22bddabe61416516e7be",
     "grade": true,
     "grade_id": "cell-3a1719cdb031d112",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "cls1 = LogisticRegression(random_state=RANDOM_STATE)\n",
    "cls2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "cls3 = GaussianNB()\n",
    "\n",
    "# voting classifier\n",
    "vcls = VotingClassifier(estimators=[('lr', cls1), ('dt', cls2), ('gauss', cls3)], voting='soft', n_jobs=-1)\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), 'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "# 10-fold cross-validation\n",
    "cv_results = cross_validate(vcls, X, y, cv=10, n_jobs=-1, scoring = scoring)\n",
    "\n",
    "avg_fmeasure = np.mean(cv_results['test_f1_score'])\n",
    "#print(\"F1: \", avg_fmeasure)\n",
    "avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "#print(\"Accuracy: \", avg_accuracy)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f0e52e5eea2eb2cb23380c80ff846cf",
     "grade": false,
     "grade_id": "cell-0ef59e80595937ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('lr',\n",
      "                              LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=42,\n",
      "                                                 solver='lbfgs', tol=0.0001,\n",
      "                                                 verbose=0, warm_start=False)),\n",
      "                             ('dt',\n",
      "                              DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features=None,\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     presort='deprecated',\n",
      "                                                     random_state=42,\n",
      "                                                     splitter='best')),\n",
      "                             ('gauss',\n",
      "                              GaussianNB(priors=None, var_smoothing=1e-09))],\n",
      "                 flatten_transform=True, n_jobs=-1, voting='soft',\n",
      "                 weights=None)\n",
      "F1-Score:0.8329794146287235 & Accuracy:0.7947965677917945\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(vcls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c7e6ab511ff3a546d2e2efb5feb892f",
     "grade": false,
     "grade_id": "cell-f6d620a3fd102626",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two estimators/classifiers. Try different classifiers for the combination of the initial classifiers. Report your results in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc7e22a168b668cbd10c524297950133",
     "grade": true,
     "grade_id": "cell-2ae5e38bd546681e",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# cls1 =  LogisticRegression(C = 1, max_iter = 100, random_state=RANDOM_STATE)\n",
    "# cls2 =  LogisticRegression(C = 0.01, max_iter = 200, random_state=RANDOM_STATE)\n",
    "\n",
    "# cls1 = DecisionTreeClassifier(max_depth = 5, random_state=RANDOM_STATE)\n",
    "# cls2 = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "cls1 =  LogisticRegression(random_state=RANDOM_STATE)\n",
    "cls2 =  DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# level-0 classifiers\n",
    "initial_estimators = [('cls1',cls1),('cls2',cls2)]\n",
    "\n",
    "# stacking classifier\n",
    "scls = StackingClassifier(estimators = initial_estimators, n_jobs=-1)\n",
    "\n",
    "# 10-fold cv\n",
    "cv_results = cross_validate(scls, X, y, cv=10, n_jobs=-1, scoring = scoring)\n",
    "\n",
    "avg_fmeasure = np.mean(cv_results['test_f1_score'])\n",
    "avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "\n",
    "# print(\"F1: \", avg_fmeasure)\n",
    "# print(\"Accuracy: \", avg_accuracy)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a27a3122627aed7d5a6f5678055f712",
     "grade": false,
     "grade_id": "cell-6d6cadab378a2b03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(cv=None,\n",
      "                   estimators=[('cls1',\n",
      "                                LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                   dual=False,\n",
      "                                                   fit_intercept=True,\n",
      "                                                   intercept_scaling=1,\n",
      "                                                   l1_ratio=None, max_iter=100,\n",
      "                                                   multi_class='auto',\n",
      "                                                   n_jobs=None, penalty='l2',\n",
      "                                                   random_state=42,\n",
      "                                                   solver='lbfgs', tol=0.0001,\n",
      "                                                   verbose=0,\n",
      "                                                   warm_start=False)),\n",
      "                               ('cls2',\n",
      "                                DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                       class_weight=None,\n",
      "                                                       criterion='gini',\n",
      "                                                       max_depth=None,\n",
      "                                                       max_features=None,\n",
      "                                                       max_leaf_nodes=None,\n",
      "                                                       min_impurity_decrease=0.0,\n",
      "                                                       min_impurity_split=None,\n",
      "                                                       min_samples_leaf=1,\n",
      "                                                       min_samples_split=2,\n",
      "                                                       min_weight_fraction_leaf=0.0,\n",
      "                                                       presort='deprecated',\n",
      "                                                       random_state=42,\n",
      "                                                       splitter='best'))],\n",
      "                   final_estimator=None, n_jobs=-1, passthrough=False,\n",
      "                   stack_method='auto', verbose=0)\n",
      "F1-Score:0.862934590550646 & Accuracy:0.8381861575178997\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6b0c745a12e384341f4e246c5242d25",
     "grade": false,
     "grade_id": "cell-8a05446ba9a944c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Report the results ###  \n",
    "Report the results of your experiments in the following cell. How did you choose your initial classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3da261e2e18ede4c057e21e080b9bac",
     "grade": true,
     "grade_id": "cell-1522ee0b7c414fba",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Το σημαντικό στο Stacking είναι οι level-0 classifiers να είναι διαφορετικοί, ώστε να κάνουν λάθος predictions σε διαφορετικά instances.\n",
    "\n",
    "Χρησιμποιώντας ως meta-classifier τον default (LogisticRegression) έγιναν τα εξής πειράματα:\n",
    "\n",
    "**1)** Mε ομογενές stacking χρησιμοποιώντας ως initial classifiers 2 Decision Trees και αλλάζοντας μόνο το max_depth έχουμε τα εξής αποτελέσματα:\n",
    "    \n",
    "  - **F1:** 0.776\n",
    "  - **Accuracy:** 0.719\n",
    "    \n",
    "**2)** Ενώ με ετερογενές stacking χρησιμοποιώντας ως initial classifiers ένα Decision Tree και ένα Logistic Regression έχουμε τα εξής αποτελέσματα:\n",
    "    \n",
    "  - **F1:**  0.863\n",
    "  - **Accuracy:**  0.838\n",
    "\n",
    "**Συμπέρασμα:** Οι ετερογονείς ensembles, λόγω του diversity στο τρόπο κατασκευής και εκπαίδευσης των επιμέρους κατηγοριοποιητών είναι πιο αποδοτικοί.\n",
    "\n",
    "Ωστόσο, ακόμα και αν χρησιμοποιήσουμε ομογενές ensemble μοντέλο μπορούμε να αυξήσουμε την απόδοση χρησιμοποιώντας **διαφορετικές παραμέτρους** στους initial classifiers. Για παράδειγμα, σε ένα μοντέλο με 2 Logistic Regression classifiers αν χρησιμοποιήσουμε στον έναν πιο μικρή τιμή για το C, δηλαδή φτιάξουμε ένα πιο γενικευμένο classifier ή αν αλλάξουμε τον αριθμό των iterations μπορούμε πάλι να κατασκευάσουμε ένα μοντέλο που συνολικά θα έχει πολύ καλή ακρίβεια. Για παράδειγμα, με τους παρακάτω initial classifiers:\n",
    "\n",
    ">**LogisticRegression(C = 1, max_iter = 100, random_state=42)**\n",
    "\n",
    ">**LogisticRegression(C = 0.01, max_iter = 200, random_state=42)**\n",
    "\n",
    "Έχουμε τα εξής αποτελέσματα που είναι τα βέλτιστα μέχρι στιγμής:\n",
    "\n",
    "- **F1:**  0.879\n",
    "- **Accuracy:**  0.857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7520f22a6a708d14fa6d56d42b78d9f",
     "grade": false,
     "grade_id": "cell-b40c3a7c4ef32588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "045df299ef8fab6007f9588af5a34d65",
     "grade": false,
     "grade_id": "cell-64c9c6881b26f5bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one is produced with a different way from the ones discussed in the lecture for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1/accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1661b594e02f8f8a73ceaad8f03b85a3",
     "grade": true,
     "grade_id": "cell-9e760b938516b506",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times:  {'Bagging': 30.99, 'Random Forest': 5.04, 'AdaBoost': 19.12, 'Decision Tree': 4.2}\n",
      "F1:  {'Bagging': 0.82, 'Random Forest': 0.857, 'AdaBoost': 0.831, 'Decision Tree': 0.748}\n",
      "Accuracy:  {'Bagging': 0.793, 'Random Forest': 0.823, 'AdaBoost': 0.802, 'Decision Tree': 0.708}\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "\n",
    "# bagging classifier\n",
    "ens1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "# random forest classifier\n",
    "ens2 = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "# adaboost classifier\n",
    "ens3 = AdaBoostClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "# a simple decision tree classifier \n",
    "tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# train-test splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# list of classifiers \n",
    "classifiers_list = [ens1, ens2, ens3, tree]\n",
    "# list of classifiers' names\n",
    "names_list = ['Bagging', 'Random Forest', 'AdaBoost', 'Decision Tree']\n",
    "\n",
    "f_measures = dict()\n",
    "accuracies = dict()\n",
    "training_times = dict()\n",
    "\n",
    "i = 0\n",
    "# process each classifier one by one\n",
    "for clf in classifiers_list:\n",
    "    # fit train data\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    # make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # round to 3 decimal points for simplicity\n",
    "    training_time = round((end - start), 2)\n",
    "    acc = round(accuracy_score(y_pred, y_test), 3)\n",
    "    f1 = round(f1_score(y_pred, y_test), 3)\n",
    "    # classifier name\n",
    "    name = names_list[i]\n",
    "    # update dictionaires\n",
    "    f_measures.update({name: f1})\n",
    "    accuracies.update({name: acc})\n",
    "    training_times.update({name: training_time})\n",
    "    i = i + 1\n",
    "\n",
    "print('Times: ', training_times)\n",
    "print('F1: ', f_measures)\n",
    "print('Accuracy: ',accuracies)\n",
    "\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da6aad44e9bd8a9f7cc06eccec886c0",
     "grade": false,
     "grade_id": "cell-77f4dc2cd4cb2f7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                        class_weight=None,\n",
      "                                                        criterion='gini',\n",
      "                                                        max_depth=None,\n",
      "                                                        max_features=None,\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=1,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        presort='deprecated',\n",
      "                                                        random_state=None,\n",
      "                                                        splitter='best'),\n",
      "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
      "                  oob_score=False, random_state=42, verbose=0,\n",
      "                  warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=42, splitter='best')\n",
      "Classifier:Bagging -  F1:0.82\n",
      "Classifier:Random Forest -  F1:0.857\n",
      "Classifier:AdaBoost -  F1:0.831\n",
      "Classifier:Decision Tree -  F1:0.748\n",
      "Classifier:Bagging -  Accuracy:0.793\n",
      "Classifier:Random Forest -  Accuracy:0.823\n",
      "Classifier:AdaBoost -  Accuracy:0.802\n",
      "Classifier:Decision Tree -  Accuracy:0.708\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1:{}\".format(name,score))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  Accuracy:{}\".format(name,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48f3eafac80f6dd15441c4991c78836d",
     "grade": false,
     "grade_id": "cell-a6ea07f0be814a40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9395efc3b936166e55b3bec6e0afdab3",
     "grade": true,
     "grade_id": "cell-399fc5e7254f1c58",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**1) Bagging classifier**\n",
    "\n",
    "Bagging ονομάζουμε τη διαδικασία κατα την οποία επιλέγουμε διαφορετικά samples με επανατοποθέτηση από τα αρχικά δεδομένα. O Bagging classifier είναι μια ensemble μέθοδος που εκπαιδεύει πολλούς classifiers (στη συγκεκριμένη περίπτωση DecisionTreeClassifier) κάθε έναν με διαφορετικά τυχαία δείγματα που δημιουργήθηκαν με τη διαδικασία του bagging. Με αυτόν τον τρόπο κάνει randomization στον τρόπο που κατασκευάζονται τα δέντρα. Στη συνέχεια, για να κάνει μια τελική πρόβλεψη ομαδοποιεί τις προβλέψεις από τους επιμέρους κατηγοριοποιητές (voting ή averaging). \n",
    "\n",
    "   - **Accuracy:** 0.782\n",
    "   \n",
    "   - **F1 Score:** 0.813\n",
    "\n",
    "**2) Random Forest classifier**\n",
    "\n",
    "Ο RandomForest βασίζεται πάλι στην μέθοδο με τα τυχαία samples του Bagging που αναφέρουμε παραπάνω. Η βασική διαφορά από τον BaggingClassifier είναι στην επιλογή των splits. O BaggingClassifier όταν καλείται να επιλέξει split point μπορεί να λάβει υπόψιν όλα τα features και να επιλέξει το optimal split. Από την άλλη ο RandomForest μπορεί να επιλέξει κάθε φορά μεταξύ m τυχαία επιλεγμένων features για να κάνει το split. Έτσι δημιουργούνται διαφορετικά υπό-μοντέλα (δέντρα με διαφορετικά splits) και με το συνδυασμό των predictions τους μπορεί να πετύχουμε καλύτερα αποτελέσματα.\n",
    "\n",
    "   - **Accuracy:** 0.829\n",
    "    \n",
    "   - **F1 Score:** 0.863\n",
    "\n",
    "**3) AdaBoost Classifier**\n",
    "\n",
    "Στον 3ο classifier χρησιμοποιούμε τη μέθοδο του Boosting, κατά την οποία εκπαιδεύουμε μοντέλα διαδοχικά και κάθε μοντέλο προσπαθεί να διορθώσει τα λάθη του προηγούμενου. Αυτό πραγματοποιείται αναθέτοντας μεγαλύτερο βάρος στα instances που έγιναν missclassified στο προηγούμενο μοντέλο. Συγκεκριμένα χρησιμοποιούμε τον Adaboost classifier με τις default παραμέτρους, δηλαδή με 50 estimators (Decision Trees με max_depth = 1).\n",
    "\n",
    "   - **Accuracy:** 0.805\n",
    "   \n",
    "   - **F1 Score:** 0.836\n",
    "\n",
    "Η απόδοση του **Decision Tree Classifier** είναι η εξής:\n",
    "\n",
    "   - **Accuracy:** 0.739\n",
    "   \n",
    "   - **F1 Score:** 0.768\n",
    "\n",
    "**Παρατηρηση:** Τα 3 ομογενή ensemble μοντέλα πετυχαίνουν σαφώς καλύτερα αποτελέσματα απο τον απλό Decision Tree Classifier. Συγκεκριμένα με τις default παραμέτρους ο RandomForest αποδίδει καλύτερα από όλους τόσο σε θέμα ακρίβειας όσο και στον χρόνο που χρειάζεται για την εκπαίδευση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8edad6471eab5b0645f6ca46bab2f7a",
     "grade": false,
     "grade_id": "cell-a0de461bc76e0880",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9573961dbe26d9ce6669df5da70a45a",
     "grade": true,
     "grade_id": "cell-0a28025407c78a48",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Με **10** estimators που είναι το default για τον Bagging classifier:\n",
    "\n",
    "   - **Training Time:** ~ 34 sec.\n",
    "    \n",
    "Με **100** estimators έχουμε:\n",
    "\n",
    "   - **Training Time:** ~ 335 sec \n",
    "    \n",
    "Βλεπουμε οτι με 100 estimators θέλουμε 10 φορές περισσότερο χρόνο για την εκπαίδευση από ότι με 10. Ωστόσο, αν είχαμε περισσότερη υπολογιστική δύναμη θα μπορούσαμε να εκπαιδεύσουμε **παράλληλα** τους επιμέρους classifiers και να συνδυάσουμε στο τέλος τα αποτελέσματα τους εξοικονομώντας έτσι χρόνο.\n",
    "\n",
    "Στους boosting classifiers δε θα μπορούσαμε να εφαρμόσουμε αυτή τη λύση, καθώς κάθε classifier περιμένει τα αποτελέσματα του προηγούμενου και εκπαιδεύεται προσπαθώντας να εστιάσει στα σημεία που έγινε λάθος classification. Αυτή η **ακολουθιακή** διαδικασία δε μας επιτρέπει να εκπαιδεύσουμε παράλληλα τους επιμέρους classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d016d23320010c4d8d9e8218cba553e8",
     "grade": false,
     "grade_id": "cell-35e46873d8c6537c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7e2a127a27e4c9131e94ae73e6e325b",
     "grade": false,
     "grade_id": "cell-6de6582e696ba2d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve an accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8d58b62ea96a7e0e0776f79c58e4b10",
     "grade": true,
     "grade_id": "cell-d1bba508731c9030",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8424769860211387\n",
      "F1:  0.8666828404382505\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "cls1 = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "cls2 = AdaBoostClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "cls3 = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "best_cls = VotingClassifier(estimators=[\n",
    "     ('rf', cls1), ('adaBoost', cls2), ('log', cls3)], voting='soft', n_jobs = -1)\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), 'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "cv_results = cross_validate(best_cls, X, y, cv=10, n_jobs=-1, scoring = scoring)\n",
    "\n",
    "best_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "print(\"Accuracy: \", best_accuracy)\n",
    "best_fmeasure = np.mean(cv_results['test_f1_score'])\n",
    "print(\"F1: \", best_fmeasure)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ecc691ce956fad47c497b9849747c34",
     "grade": false,
     "grade_id": "cell-39673f451b660dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('rf',\n",
      "                              RandomForestClassifier(bootstrap=True,\n",
      "                                                     ccp_alpha=0.0,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     max_samples=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators=100,\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score...\n",
      "                                                 learning_rate=1.0,\n",
      "                                                 n_estimators=50,\n",
      "                                                 random_state=42)),\n",
      "                             ('log',\n",
      "                              LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=42,\n",
      "                                                 solver='lbfgs', tol=0.0001,\n",
      "                                                 verbose=0,\n",
      "                                                 warm_start=False))],\n",
      "                 flatten_transform=True, n_jobs=-1, voting='soft',\n",
      "                 weights=None)\n",
      "F1-Score:0.8666828404382505 & Accuracy:0.8424769860211387\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(best_cls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(best_fmeasure,best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "667632db5afb6f143062507bae31063f",
     "grade": false,
     "grade_id": "cell-6a072817c64ce4a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3abcb52a70fcb6da4f93980026b8e593",
     "grade": true,
     "grade_id": "cell-5f1d5ba45ffeb074",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Για να υλοποιήσω τον τελικό classifier ακολούθησα την εξής διαδικασία:\n",
    "\n",
    "Αρχικά, δοκίμασα έναν RandomForest classifier που πέτυχε τα καλύτερα αποτελέσματα στην προηγούμενη άσκηση. Με την πρώτη προσπάθεια πετύχαμε τα εξής αποτελέσματα με ακρίβεια 3 δεκαδικών ψηφίων:\n",
    "\n",
    "   >**- Accuracy:** 0.809\n",
    "\n",
    "   >**- F1 Score:** 0.847\n",
    "\n",
    "Δοκίμασα να αλλάξω τις παραμέτρους του RandomForest, άλλα η ακρίβεια παρέμεινε στα ίδια επίπεδα. Για να πλησιάσουμε στα επιθυμητά αποτελέσματα (83-84% accuracy) δοκιμάζουμε να προσθέσουμε ένα ακόμα classifier που υλοποιείται με διαφορετικό τρόπο, ώστε να κάνει λάθη σε διαφορετικά σημεία. Έχοντας ήδη έναν classifier που υλοποιεί την στρατηγική του **Bagging**, θα προσθέσουμε μια μέθοδο **boosting**. Δημιουργούμε έναν **voting classifier** με τον Random Forest και τον Adaboost ως τους 2 επιμέρους clasifiers του μοντέλου. Πλέον έχουμε τα εξής αποτελέσματα:\n",
    "\n",
    "   >**- Accuracy:**  0.811\n",
    "    \n",
    "   >**- F1 Score:**  0.848\n",
    "\n",
    "Παρατηρούμε μια μικρή βελτίωση στο μοντέλο μας, αλλά είμαστε ακόμα μακρυά από τον στόχο μας. Ένας τρόπος να αυξήσουμε και άλλο την ακρίβεια του μοντέλου είναι να προσθέσουμε έναν ακόμα διαφορετικό classifier. Μέχρι στιγμής τόσο ο Random Forest όσο και ο Adaboost βασίζονται σε δεντρικές μεθόδους. Προσθέτουμε λοιπόν έναν classifier που βασίζεται σε γραμμική μέθοδο. Προσθέτουμε έναν **LogisticRegression** classifier στον voting classifier. Τα νέα αποτελέσματα:\n",
    "\n",
    "   >**- Accuracy:**  0.839\n",
    "    \n",
    "   >**- F1 Score:**  0.864\n",
    "\n",
    "Προσθέτοντας και 4ο classifier μπορούμε να αυξήσουμε ακόμα περισσότερο την απόδοση του μοντέλου, αλλά ταυτόχρονα αυξάνεται αρκετά και ο χρόνος εκπαίδευσης. Έχοντας φτάσει πλέον το επιθυμητό αποτέλεσμα δε θα επεκτείνουμε περαιτέρω το μοντέλο μας.\n",
    "\n",
    "**Παρατήρηση:** όσον αφορά τις παραμέτρους του τελικού **voting classifier**, χρησιμοποιήσαμε την πολιτική του **'soft voting'** καθώς μας έδωδε καλύτερα αποτελέσματα.\n",
    "\n",
    "**!!!Τα αποτελέσματα ήταν λίγο διαφορετικά όταν έκανα restart τον kernel. Ισως κάπου έπρεπε να ορίσω random state και το παρέλειψα!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22500bda285c8dc9375ee048e883be55",
     "grade": false,
     "grade_id": "cell-5b27d068d1fbfa37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c40bf6a2d6e630b217742246c20d2560",
     "grade": true,
     "grade_id": "cell-ab69a2863e87fd72",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
    "test_set.head()\n",
    "\n",
    "cls1 = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "cls2 = AdaBoostClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "cls3 = LogisticRegression(random_state=RANDOM_STATE,  max_iter=1000)\n",
    "\n",
    "cls = VotingClassifier(estimators=[('rf', cls1),('adaBoost', cls2), ('log', cls3)], voting='soft', n_jobs=-1)\n",
    "\n",
    "cls.fit(X,y)\n",
    "\n",
    "predictions = cls.predict(test_set)\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace9dbe06e5607ddf9353befef8472c0",
     "grade": false,
     "grade_id": "cell-d98d6687c3bbe4ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('rf',\n",
      "                              RandomForestClassifier(bootstrap=True,\n",
      "                                                     ccp_alpha=0.0,\n",
      "                                                     class_weight=None,\n",
      "                                                     criterion='gini',\n",
      "                                                     max_depth=None,\n",
      "                                                     max_features='auto',\n",
      "                                                     max_leaf_nodes=None,\n",
      "                                                     max_samples=None,\n",
      "                                                     min_impurity_decrease=0.0,\n",
      "                                                     min_impurity_split=None,\n",
      "                                                     min_samples_leaf=1,\n",
      "                                                     min_samples_split=2,\n",
      "                                                     min_weight_fraction_leaf=0.0,\n",
      "                                                     n_estimators=100,\n",
      "                                                     n_jobs=None,\n",
      "                                                     oob_score...\n",
      "                                                 learning_rate=1.0,\n",
      "                                                 n_estimators=50,\n",
      "                                                 random_state=42)),\n",
      "                             ('log',\n",
      "                              LogisticRegression(C=1.0, class_weight=None,\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=1000,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=42,\n",
      "                                                 solver='lbfgs', tol=0.0001,\n",
      "                                                 verbose=0,\n",
      "                                                 warm_start=False))],\n",
      "                 flatten_transform=True, n_jobs=-1, voting='soft',\n",
      "                 weights=None)\n",
      "[1 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(cls)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1af1d441fd486d53e45173d4f352ba8c",
     "grade": false,
     "grade_id": "cell-966633c679d5c960",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "LEAVE HERE ANY COMMENTS ABOUT YOUR CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για τα predictions χρησιμοποιήθηκε ο classifier που περιγράφεται στο **3.1** με τον οποίον πετύχαμε ακρίβεια κοντά στο **84%** με 10-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24dbe2151df25b6e5b36e988a8e38dcb",
     "grade": false,
     "grade_id": "cell-78ffc0c68225fb1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddcf51aaeaaa305540873fd0012a4b06",
     "grade": false,
     "grade_id": "cell-7946d9ee342bf549",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "final_test_set = pd.read_csv('test_set.csv')\n",
    "ground_truth = final_test_set['CLASS']\n",
    "print(\"Accuracy:{}\".format(accuracy_score(predictions,ground_truth)))\n",
    "print(\"F1-Score:{}\".format(f1_score(predictions,ground_truth)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
